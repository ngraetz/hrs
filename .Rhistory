exp(0.39)
wilcox.test(log(treatment),log(control),conf.int=TRUE)
## 2. (g) Wilcoxon, multiplicative
## Wilcoxon statistic should be identical under additive and multiplicative models, right?
wilcox_log_approx <- wilcox.test(log(treatment),log(control),alternative="greater")$p.value
wilcox_log_approx
wilcox.test(log(treatment),log(control),alternative="greater")
wilcox.test(log(treatment),log(control),conf.int=TRUE)
wilcox.test(log(treatment),log(control),alternative="greater",conf.int=TRUE)
wilcox.test(log(treatment),log(control),alternative="greater")
wilcox.test(log(treatment),log(control),alternative="greater",conf.int=TRUE)
wilcox.test(log(treatment),log(control),conf.int=TRUE)
exp(1.26)
wilcox.test(log(treatment),log(control),alternative="greater")
wilcox.test(log(treatment),log(control),conf.int=TRUE)
## 2. (g) Wilcoxon, multiplicative
## Wilcoxon statistic should be identical under additive and multiplicative models, right?
wilcox.test(log(treatment),log(control),alternative="greater")$p.value
wilcox.exact(log(treatment),log(control),alternative="greater")$p.value
## Fisher null, multiplicative
t.test(log(treatment),log(control),var.equal=TRUE,alternative="greater")
5.134180 - 3.989723
## 2. (f) Compare Fisher's sharp null additive model with multiplicative treatment model.
t.test(log(treatment),log(control),var.equal=TRUE,alternative="greater")
## 2. (f) Compare Fisher's sharp null additive model with multiplicative treatment model.
mult_test <- t.test(log(treatment),log(control),var.equal=TRUE,alternative="greater")
mult_test$estimate
mult_test$parameter
class(mult_test$estimate)
length(mult_test$estimate)
mult_test$estimate[1] - mult_test$estimate[2]
mult_fisher_p <- mult_test$p.value
mult_fisher_effect <- exp(mult_test$estimate[1] - mult_test$estimate[2])
mult_fisher_p <- mult_test$p.value
## 2. (g) Wilcoxon, multiplicative
## Wilcoxon statistic should be identical under additive and multiplicative models, right?
wilcox.test(log(treatment),log(control),conf.int=TRUE)
## 2. (g) Wilcoxon, multiplicative
mult_wilcoxon <- wilcox.test(log(treatment),log(control),conf.int=TRUE)
effect_upper <- mult_wilcoxon$conf.int
mult_fisher_effect <- mult_test$estimate[1] - mult_test$estimate
effect_ci
effect_ci <- mult_wilcoxon$conf.int
## 2. (f) Compare Fisher's sharp null additive model with multiplicative treatment model.
mult_fisher <- t.test(log(treatment),log(control),var.equal=TRUE,alternative="greater")
mult_fisher
mult_wilcoxon <- wilcox.test(log(treatment),log(control),conf.int=TRUE)
effect_ci <- mult_wilcoxon$conf.int
mult_wilcoxon <- wilcox.test(log(treatment),log(control),conf.int=TRUE)
effect_ci <- mult_wilcoxon$conf.int
mult_wilcoxon
exp(1.14)
?t.test
class(treat.effect.samplemean.montecarlo.test.func(treatment,control,10000)
)
length(treat.effect.samplemean.montecarlo.test.func(treatment,control,10000)
)
data.frame(pval=pval,lowerci=lowerci,upperci=upperci);
treat.effect.samplemean.montecarlo.test.func=function(treated.r,control.r,K){
# Create vectors for r and Z, and find total number in
# experiment and number of treated subjects
r=c(treated.r,control.r);
Z=c(rep(1,length(treated.r)),rep(0,length(control.r)));
N=length(r);
m=length(treated.r);
# Observed test statistic
obs.test.stat=mean(r[Z==1])-mean(r[Z==0]);
# Monte Carlo simulatoin
montecarlo.test.stat=rep(0,K);
for(i in 1:K){
treatedgroup=sample(1:N,m);  # Draw random assignment
controlgroup=(1:N)[-treatedgroup];
# Compute test statistic for random assignment
montecarlo.test.stat[i]=mean(r[treatedgroup])-mean(r[controlgroup]);
}
# Monte Carlo p-value is proportion of randomly drawn
# test statistics that are >= observed test statistic
pval=sum(montecarlo.test.stat>=obs.test.stat)/K;
# 95% CI for true p-value based on Monte Carlo p-value
lowerci=pval-1.96*sqrt(pval*(1-pval)/K);
upperci=pval+1.96*sqrt(pval*(1-pval)/K);
data.frame(pval=pval,lowerci=lowerci,upperci=upperci);
}
treat.effect.samplemean.montecarlo.test.func(treatment,control,10000)
treat.effect.samplemean.montecarlo.test.func=function(K,treated.r,control.r){
# Create vectors for r and Z, and find total number in
# experiment and number of treated subjects
r=c(treated.r,control.r);
Z=c(rep(1,length(treated.r)),rep(0,length(control.r)));
N=length(r);
m=length(treated.r);
# Observed test statistic
obs.test.stat=mean(r[Z==1])-mean(r[Z==0]);
# Monte Carlo simulatoin
montecarlo.test.stat=rep(0,K);
for(i in 1:K){
treatedgroup=sample(1:N,m);  # Draw random assignment
controlgroup=(1:N)[-treatedgroup];
# Compute test statistic for random assignment
montecarlo.test.stat[i]=mean(r[treatedgroup])-mean(r[controlgroup]);
}
# Monte Carlo p-value is proportion of randomly drawn
# test statistics that are >= observed test statistic
pval=sum(montecarlo.test.stat>=obs.test.stat)/K;
# 95% CI for true p-value based on Monte Carlo p-value
lowerci=pval-1.96*sqrt(pval*(1-pval)/K);
upperci=pval+1.96*sqrt(pval*(1-pval)/K);
data.frame(pval=pval,lowerci=lowerci,upperci=upperci);
}
p_cis <- rbindlist(lapply(seq(1000,20000,1000), treat.effect.samplemean.montecarlo.test.func, treated.r = treatment, control.r = control))
?geom_ribbon
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci))
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci))
p_cis$sims <- seq(1000,20000,1000)
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci))
p_cis <- rbindlist(lapply(seq(1000,100000,1000), treat.effect.samplemean.montecarlo.test.func, treated.r = treatment, control.r = control))
p_cis$sims <- seq(1000,20000,1000)
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci))
p_cis$sims <- seq(1000,100000,1000)
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci))
p_cis <- rbindlist(lapply(seq(1000,10000,100), treat.effect.samplemean.montecarlo.test.func, treated.r = treatment, control.r = control))
p_cis$sims <- seq(1000,10000,100)
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci))
ggplot(data=p_cis) +
geom_ribbon(aes(x=sims,
ymin=lowerci,
ymax=upperci)) +
labs(title = 'Confidence interval on p-value based on number of simulations')
# mult_fisher <- t.test(log(treatment),log(control),var.equal=TRUE,alternative="greater")
# mult_fisher_effect <- mult_test$estimate[1] - mult_test$estimate
# mult_fisher_p <- mult_test$p.value
treat.effect.samplemean.montecarlo.test.func(10000,log(treatment),log(control))
mean(log(treatment) - log(control))
effect_ci
mult_wilcoxon$estimate
wilcox.test(log(treatment),log(control),conf.int=TRUE)$estimate
class(wilcox.test(log(treatment),log(control),conf.int=TRUE)$estimate)
# Median of the difference between a sample from x and a sample from y.
exp(wilcox.test(log(treatment),log(control),conf.int=TRUE)$estimate)
exp(mean(log(treatment) - log(control)))
log(treatment) - log(control)
mean(log(treatment) - log(control))
wilcox.test(log(treatment),log(control),conf.int=TRUE)
2^8
2^12
2^15
2^14
# Load knitr package and settings
library(knitr)
library(data.table)
library(ggplot2)
library(formatR)
options(scipen=999)
#opts_chunk$set(fig.align='center', tidy=TRUE, tidy.opts=list(blank=TRUE, width.cutoff=40), warning=FALSE,message=FALSE)
#opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
## Run test and calculate CI.
wilcox.test(d$treatment,d$control,paired=TRUE,alternative="less")
## Set up some data and functions.
set.seed(1234)
d <- fread('hw_2.csv')
## Run test and calculate CI.
wilcox.test(d$treatment,d$control,paired=TRUE,alternative="less")
police <- data.table(beat=c('u','u','u','m','m','m','i','i','i'),
length=c(5,10,15,5,10,15,5,10,15),
score=c(34.4,35.5,39.2,30.2,32.4,34.7,20.1,39.4,54.3))
## Interaction plot (x=length,y=score,color=beat)
ggplot() +
geom_line(data=police,
aes(x=length,
y=score,
color=beat)) +
labs(x='Length of Human Relations Course',y='Attitude test score',title='Interaction plot of Human Relations Course effect on attitude scores by officer beat assignment') +
theme_minimal()
pi
10*(pi*(r1^2))
r1 <- 10
10*(pi*(r1^2))
(pi*(r1^2))
solve((pi*(r^2))=(pi*(r1^2))/10,r)
?solve
r1 <- 10
area1 <- pi*(r1^2)
r2 <- sqrt((area1/10)/pi)
r3 <- sqrt((area1/100)/pi)
r4 <- sqrt((area1/1000)/pi)
1000000/50000
# Load knitr package and settings
library(knitr)
library(data.table)
library(ggplot2)
library(formatR)
options(scipen=999)
#opts_chunk$set(fig.align='center', tidy=TRUE, tidy.opts=list(blank=TRUE, width.cutoff=40), warning=FALSE,message=FALSE)
#opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
d <- expand.grid(c('8_hours','13_hours'), c('Old','New'), c('Low','High'))
d
d <- as.data.table(expand.grid(c('8_hours','13_hours'), c('Old','New'), c('Low','High')))
mean(.83,.78)
d <- as.data.table(expand.grid(c('8_hours','13_hours'), c('Old','New'), c('Low','High')))
names(d) <- c('time','laser','airflow')
d[, y := c(mean(c(.83,.78)),mean(c(.18,.16)),mean(c(.86,.67)),mean(c(.30,.23)),
mean(c(.68,.90)),mean(c(.25,.20)),mean(c(.72,.81)),mean(c(.10,.14)))]
d
model1 <- glm(Y ~ time * laser * airflow, data = d)
model1 <- glm(y ~ time * laser * airflow, data = d)
model1
summary(model1)
model1 <- lm(y ~ time * laser * airflow, data = d)
model2 <- glm(y ~ time * laser * airflow, family = 'binomial', data = d)
model1
summary(model1)
summary(model2)
summary(model1)
d <- as.data.table(expand.grid(c('8_hours','13_hours'), c('Old','New'), c('Low','High')))
names(d) <- c('time','laser','airflow')
d[, y := c(mean(c(.83,.78)),mean(c(.18,.16)),mean(c(.86,.67)),mean(c(.30,.23)),
mean(c(.68,.90)),mean(c(.25,.20)),mean(c(.72,.81)),mean(c(.10,.14)))]
d[time=='8_hours', time_i := -1]
d[time=='13_hours', time_i := 1]
d[laser=='Old', laser_i := -1]
d[laser=='New', laser_i := 1]
d[airflow=='Low', airflow_i := -1]
d[airflow=='High', airflow_i := 1]
model1 <- lm(y ~ time_i * laser_i * airflow_i, data = d)
summary(model1)
d <- as.data.table(expand.grid(c('8_hours','13_hours'), c('Old','New'), c('Low','High')))
names(d) <- c('time','laser','airflow')
d <- rbind(d,d)
d[, y := c(.83,.18,.86,.30,.68,.25,.72,.10,.78,.16,.67,.23,.90,.20,.81,.14)]
d
d <- as.data.table(expand.grid(c('8_hours','13_hours'), c('Old','New'), c('Low','High')))
names(d) <- c('time','laser','airflow')
d <- rbind(d,d)
d[, y := c(.83,.18,.86,.30,.68,.25,.72,.10,.78,.16,.67,.23,.90,.20,.81,.14)]
d[time=='8_hours', time_i := -1]
d[time=='13_hours', time_i := 1]
d[laser=='Old', laser_i := -1]
d[laser=='New', laser_i := 1]
d[airflow=='Low', airflow_i := -1]
d[airflow=='High', airflow_i := 1]
model1 <- lm(y ~ time_i * laser_i * airflow_i, data = d)
summary(model1)
anova(y ~ time_i * laser_i * airflow_i, data = d)
?anova
anova(model1)
summary(model1)
eff=2*coef(regmodel)[-1]
eff=2*coef(model1)[-1]
eff
?wilcox.test
d <- fread('hw_2.csv')
d
## Run rank sum test and calculate CI.
wilcox.test(d$treatment,d$control,paired=FALSE,alternative="less")
## Run rank sum test and calculate CI.
wilcox.test(d$treatment,d$control,paired=FALSE,alternative="less",conf.int=TRUE)
d
d <- melt(d, id.vars = 'i', measure.vars = c('treatment','control'), variable.name = 'Type')
d
d <- fread('hw_2.csv')
setnames(d, c('treatment','control'), c('Marijuana','Placebo'))
d <- melt(d, id.vars = 'i', measure.vars = c('Marijuana','Placebo'), variable.name = 'Group', value.name = 'Vomiting')
d
d <- fread('hw_2.csv')
setnames(d, c('treatment','control'), c('Marijuana','Placebo'))
d <- melt(d, id.vars = 'i', measure.vars = c('Marijuana','Placebo'), variable.name = 'Group', value.name = 'Vomiting')
d[, type := 'Vomiting']
## Check additive vs. multiplicative treatment effect model.
plot_log <- copy(d)
plot_log[, Vomiting := log(Vomiting)]
plot_log[, Type := 'Log vomiting']
plot <- rbind(plot, plot_log)
d <- fread('hw_2.csv')
setnames(d, c('treatment','control'), c('Marijuana','Placebo'))
d <- melt(d, id.vars = 'i', measure.vars = c('Marijuana','Placebo'), variable.name = 'Group', value.name = 'Vomiting')
d[, type := 'Vomiting']
## Check additive vs. multiplicative treatment effect model.
plot_log <- copy(d)
plot_log[, Vomiting := log(Vomiting)]
plot_log[, Type := 'Log vomiting']
plot <- rbind(d, plot_log)
d
plot_log
d <- fread('hw_2.csv')
## Run rank sum test and calculate CI.
wilcox.test(d$treatment,d$control,paired=TRUE,alternative="less",conf.int=TRUE)
## Run rank sum test and calculate CI.
wilcox.test(d$treatment,d$control, conf.int=TRUE)
## Run rank sum test and calculate CI.
wilcox.test(d$treatment,d$control,alternative='less',conf.int=TRUE)
## Run rank sum test and calculate CI.
wilcox.test(d$treatment,d$control,paired=TRUE,conf.int=TRUE)
wilcox.test(d$treatment,d$control,paired=TRUE,alternative="less",conf.int=TRUE)
## Run rank sum test and calculate CI.
m1 <- wilcox.test(d$treatment,d$control,paired=TRUE,conf.int=TRUE)
m1$conf.int
m1$parameter
m1$estimate
ci[1,]
## Run rank sum test and calculate CI.
ci <- wilcox.test(d$treatment,d$control,paired=TRUE,conf.int=TRUE)$conf.int
ci[,1]
ci[1]
set.seed(1234)
setwd('C:/Users/ngraetz/Documents/repos/causal_inference')
d <- fread('hw_2.csv')
## Run rank sum test and calculate CI.
ci <- wilcox.test(d$treatment,d$control,paired=TRUE,conf.int=TRUE)$conf.int
m <- wilcox.test(d$treatment,d$control,paired=TRUE,alternative="less",conf.int=TRUE)$estimate
message(paste0(m, '(',round(ci[1]), ' - ', round(ci[2]), ')'))
paste0(m, '(',round(ci[1]), ' - ', round(ci[2]), ')')
police <- data.table(beat=c('Upper','Upper','Upper','Middle','Middle','Middle','Inner','Inner','Inner'),
length=c(5,10,15,5,10,15,5,10,15),
score=c(34.4,35.5,39.2,30.2,32.4,34.7,20.1,39.4,54.3))
## Interaction plot (x=length,y=score,color=beat)
ggplot() +
geom_line(data=police,
aes(x=length,
y=score,
color=beat)) +
labs(x='Length of Human Relations Course',y='Attitude test score',title='Interaction plot of Human Relations Course effect on attitude scores by officer beat assignment') +
theme_minimal()
## Interaction plot (x=length,y=score,color=beat)
ggplot() +
geom_line(data=police,
aes(x=length,
y=score,
color=beat), size=2) +
labs(x='Length of Human Relations Course',y='Attitude test score',title='Interaction plot of Human Relations Course effect on attitude scores by officer beat assignment') +
theme_minimal()
## Set up some data and functions.
set.seed(1234)
setwd('C:/Users/ngraetz/Documents/repos/causal_inference')
d <- fread('hw_2.csv')
## Run rank sum test and calculate CI.
ci <- wilcox.test(d$treatment,d$control,paired=TRUE,conf.int=TRUE)$conf.int
m <- wilcox.test(d$treatment,d$control,paired=TRUE,alternative="less",conf.int=TRUE)$estimate
message(paste0(round(m), ' (',round(ci[1]), ' - ', round(ci[2]), ')'))
m
ci
install.packages('sjPlot')
library(sjPlot)
devtools::install_github("strengejacke/strengejacke
)
')
)
)
)
)
0
)
))
0
0
)
)
0'
''
''
library(data.table)
library(ggplot2)
library(formatR)
library(dplyr)
library(gridExtra)
library(lme4)
library(survey)
library(mice)
library(survey)
library(purrr)
library(stargazer)
library(sjPlot)
repo <- 'C:/Users/ngraetz/Documents/repos/hrs'
setwd(repo)
hrs <- readRDS('hrs_imputed_v2.RDS')
outcome_var <- 'cognitive'
REML <- FALSE
weight <- FALSE
ses <- FALSE
if(ses) numeric_vars <- c('edu_years','wealth','log_income')
if(!ses) numeric_vars <- NULL
numeric_vars <- 'edu_years'
factor_vars <- c('as.factor(female)','as.factor(cohort_group)')
## Calculate baseline cognition and only keep those where we observe cognition at "baseline" (before age 60)
model_hrs <- copy(hrs)
model_hrs[, age_obs := age]
model_hrs[, age_obs := min(age_obs, na.rm = T), by = id]
model_hrs[age==age_obs, baseline_cog := get(outcome_var)]
model_hrs[, baseline_cog := max(baseline_cog, na.rm = T), by = id] ## Repeat within individual
model_hrs <- model_hrs[age_obs < 60, ]
model_hrs[baseline_cog<15, baseline := 'low']
model_hrs[baseline_cog %in% 15:19, baseline := 'middle']
model_hrs[baseline_cog %in% 20:35, baseline := 'high']
## Subset to respondents with at least 3 responses on outcome.
id_nums <- model_hrs[, .N , by = id]
dim(id_nums[N == 1, ])
model_hrs <- model_hrs[id %in% id_nums[N > 3, id], ]
# model_hrs[, id_factor := as.factor(id)]
## Rescale all variables to mean/sd=0/1. Keep track of means/sds to transform back after predicting.
scale_vars <- c(outcome_var, numeric_vars, 'baseline_cog')
means <- model_hrs[, lapply(.SD, mean, na.rm=TRUE), .SDcols=scale_vars]
means <- melt(means, measure.vars=scale_vars)
sds <- model_hrs[, lapply(.SD, sd, na.rm=TRUE), .SDcols=scale_vars]
sds <- melt(sds, measure.vars=scale_vars)
setnames(sds, 'value', 'sd')
setnames(means, 'value', 'mean')
scales <- merge(means,sds)
model_hrs <- as.data.table(model_hrs %>% mutate_at(funs(scale(.) %>% as.vector), .vars=scale_vars)) # Subtract mean, divide by SD.
## Center linear age at 50 to interpret baseline intercept
model_hrs[, age := (age - 50) / 5]
## If we want age polynomials, we need to calculate orthogonal polynomials. Because our ages are relatively high, age and age^2
## are basically perfectly correlated so the model can't converge.
age_polys <- poly(model_hrs[, age], degree = 2)
model_hrs[, age_poly_1 := age_polys[,1]]
model_hrs[, age_poly_2 := age_polys[,2]]
model_hrs <- as.data.table(model_hrs %>% mutate_at(funs(scale(.) %>% as.vector), .vars=c('age_poly_1','age_poly_2'))) # Subtract mean, divide by SD.
## Fit model, pull out random effects to calculate individual trajectories. Coef() gives random + fixed effects.
model_hrs[, race := factor(race, levels=c('white','black','hispanic','other'))]
model_hrs[, id_factor := as.factor(id)]
model_hrs <- model_hrs[!is.infinite(baseline_cog), ]
model_hrs <- model_hrs[!is.na(pweight) & pweight > 0, ]
model_hrs[, pweight := pweight / 10000]
formula1 <- as.formula(paste0(outcome_var, ' ~ ', paste(c(factor_vars), collapse=' + '), ' + age*baseline_cog*race + (age||id_factor)'))
model1 <- lmer(formula1, weights=pweight, data=model_hrs, REML=FALSE)
formula2 <- as.formula(paste0(outcome_var, ' ~ ', paste(c(factor_vars), collapse=' + '), ' + baseline_cog + age*edu_years*race + (age||id_factor)'))
model2 <- lmer(formula2, weights=pweight, data=model_hrs, REML=FALSE)
tab_model(list(model1,model2), dv.labels = c('Model 1','Model 2'), show.ci=FALSE, p.style='asterisk', file='three_way_interaction.html')
head(model_hrs)
library(data.table)
library(ggplot2)
library(formatR)
library(dplyr)
library(gridExtra)
library(lme4)
library(survey)
library(mice)
library(survey)
library(purrr)
library(stargazer)
library(sjPlot)
repo <- 'C:/Users/ngraetz/Documents/repos/hrs'
setwd(repo)
hrs <- readRDS('hrs_imputed_v2.RDS')
outcome_var <- 'cognitive'
REML <- FALSE
weight <- FALSE
ses <- FALSE
if(ses) numeric_vars <- c('edu_years','wealth','log_income')
if(!ses) numeric_vars <- NULL
numeric_vars <- 'edu_years'
factor_vars <- c('as.factor(female)','as.factor(cohort_group)')
## Calculate baseline cognition and only keep those where we observe cognition at "baseline" (before age 60)
model_hrs <- copy(hrs)
model_hrs[, age_obs := age]
model_hrs[, age_obs := min(age_obs, na.rm = T), by = id]
model_hrs[age==age_obs, baseline_cog := get(outcome_var)]
model_hrs[, baseline_cog := max(baseline_cog, na.rm = T), by = id] ## Repeat within individual
model_hrs <- model_hrs[age_obs < 60, ]
model_hrs[baseline_cog<15, baseline := 'low']
model_hrs[baseline_cog %in% 15:19, baseline := 'middle']
model_hrs[baseline_cog %in% 20:35, baseline := 'high']
model_hrs <- model_hrs[!is.infinite(baseline_cog), ]
## Subset to respondents with at least 3 responses on outcome.
id_nums <- model_hrs[, .N , by = id]
dim(id_nums[N == 1, ])
model_hrs <- model_hrs[id %in% id_nums[N > 3, id], ]
# model_hrs[, id_factor := as.factor(id)]
## Rescale all variables to mean/sd=0/1. Keep track of means/sds to transform back after predicting.
scale_vars <- c(outcome_var, numeric_vars, 'baseline_cog')
means <- model_hrs[, lapply(.SD, mean, na.rm=TRUE), .SDcols=scale_vars]
means <- melt(means, measure.vars=scale_vars)
sds <- model_hrs[, lapply(.SD, sd, na.rm=TRUE), .SDcols=scale_vars]
sds <- melt(sds, measure.vars=scale_vars)
setnames(sds, 'value', 'sd')
setnames(means, 'value', 'mean')
scales <- merge(means,sds)
model_hrs <- as.data.table(model_hrs %>% mutate_at(funs(scale(.) %>% as.vector), .vars=scale_vars)) # Subtract mean, divide by SD.
## Center linear age at 50 to interpret baseline intercept
model_hrs[, age := (age - 50) / 5]
## If we want age polynomials, we need to calculate orthogonal polynomials. Because our ages are relatively high, age and age^2
## are basically perfectly correlated so the model can't converge.
age_polys <- poly(model_hrs[, age], degree = 2)
model_hrs[, age_poly_1 := age_polys[,1]]
model_hrs[, age_poly_2 := age_polys[,2]]
model_hrs <- as.data.table(model_hrs %>% mutate_at(funs(scale(.) %>% as.vector), .vars=c('age_poly_1','age_poly_2'))) # Subtract mean, divide by SD.
## Fit model, pull out random effects to calculate individual trajectories. Coef() gives random + fixed effects.
model_hrs[, race := factor(race, levels=c('white','black','hispanic','other'))]
model_hrs[, id_factor := as.factor(id)]
model_hrs <- model_hrs[!is.na(pweight) & pweight > 0, ]
model_hrs[, pweight := pweight / 10000]
formula1 <- as.formula(paste0(outcome_var, ' ~ ', paste(c(factor_vars), collapse=' + '), ' + age*baseline_cog*race + (age||id_factor)'))
model1 <- lmer(formula1, weights=pweight, data=model_hrs, REML=FALSE)
formula2 <- as.formula(paste0(outcome_var, ' ~ ', paste(c(factor_vars), collapse=' + '), ' + baseline_cog + age*edu_years*race + (age||id_factor)'))
model2 <- lmer(formula2, weights=pweight, data=model_hrs, REML=FALSE)
tab_model(list(model1,model2), dv.labels = c('Model 1','Model 2'), show.ci=FALSE, p.style='asterisk', file='three_way_interaction.html')
